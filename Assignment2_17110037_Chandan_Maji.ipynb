{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2-17110037-Chandan_Maji.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CM747/Book-Listing-app/blob/master/Assignment2_17110037_Chandan_Maji.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDis9x9QVCQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "import operator\n",
        "import string\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PukOpyp_uU4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3b1ce8c0-d5c0-4bb5-8652-146456e77cd8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDEmET9QX8aL",
        "colab_type": "text"
      },
      "source": [
        "# **DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuyHpYCBVmfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fp = open(\"/content/drive/My Drive/speeches.txt\", 'r', encoding=\"utf-8-sig\", errors='ignore')\n",
        "data = fp.read()\n",
        "data = data.lower()\n",
        "\n",
        "# DATA Preprocessing\n",
        "data = data.replace(\"\\n\", \"\")\n",
        "data = data.replace(\"...\", \". \")\n",
        "data = re.sub(r\"SPEECH [0-9]\",\"\", data)\n",
        "data = re.sub(r\"[:;]\",\".\",data)\n",
        "\n",
        "# Sentence Tokenizer\n",
        "sent_tokenize_list = sent_tokenize(data)\n",
        "\n",
        "lines = list()\n",
        "for i in range(len(sent_tokenize_list)):\n",
        "  if(len(sent_tokenize_list[i])>1):\n",
        "    lines.append(\"<s> \" + sent_tokenize_list[i][:-1] + \" </s>\")\n",
        "\n",
        "# Train Test Split\n",
        "random.shuffle(lines)\n",
        "cut = int(0.8*len(lines))\n",
        "train = lines[:cut]\n",
        "test = lines[cut:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQhVxoTqYHq7",
        "colab_type": "text"
      },
      "source": [
        "# CLASSICAL APPROACH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9c5V5VO_rAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70322bae-cbb1-4295-c912-1ec2010cf1e1"
      },
      "source": [
        "vocab_dict = {}\n",
        "vocab_len = 0\n",
        "for i in range(len(train)):\n",
        "  tokens = train[i].split(\" \")\n",
        "  for j in tokens:\n",
        "    if(j in vocab_dict):\n",
        "      vocab_dict[j] += 1\n",
        "    else:\n",
        "      vocab_dict[j] = 1\n",
        "      vocab_len += 1\n",
        "  \n",
        "tokens = sum(vocab_dict.values())\n",
        "vocab_len"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4_pf9XzKHMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d22cdb5-1a86-4a52-b5cb-bc116b3e44e1"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153331"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl553GEqFQgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to extract n_grams and their frequencies\n",
        "def ngramCounts(lines, n_gram):\n",
        "  counts = dict()\n",
        "  for i in range(len(lines)):\n",
        "    tokens = lines[i].split(\" \")\n",
        "    for j in range(0,len(tokens)-n_gram+1,1):\n",
        "      k = tuple(tokens[j: j+n_gram])\n",
        "      if k in counts:\n",
        "        counts[k] += 1\n",
        "      else:\n",
        "        counts[k] = 1\n",
        "  return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP22k20iYOMA",
        "colab_type": "text"
      },
      "source": [
        "## Maximum Likelihood Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFjpbihQKLwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to get Maximum Likelihood Estimate Probabilities for unigrams, bigrams, trigrams and quadgrams\n",
        "count_unigram = ngramCounts(lines, 1)\n",
        "count_bigram = ngramCounts(lines, 2)\n",
        "count_trigram = ngramCounts(lines, 3)\n",
        "count_quadgram = ngramCounts(lines, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcViyiWcfGLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unigramMLE(word_list):\n",
        "  if(tuple(word_list) in count_unigram):\n",
        "    return (count_unigram[tuple(word_list)])/(tokens)\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def bigramMLE(word_list):\n",
        "  if(tuple(word_list) in count_bigram):\n",
        "    return (count_bigram[tuple(word_list)])/(count_unigram[tuple(word_list[:-1])])\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def trigramMLE(word_list):\n",
        "  if(tuple(word_list) in count_trigram):\n",
        "    return (count_trigram[tuple(word_list)])/(count_bigram[tuple(word_list[:-1])])\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def quadgramMLE(word_list):\n",
        "  if(tuple(word_list) in count_quadgram):\n",
        "    return (count_quadgram[tuple(word_list)])/(count_trigram[tuple(word_list[:-1])])\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXkJY2Y_g-az",
        "colab_type": "code",
        "outputId": "21bd9dd9-fa84-446d-baa8-64c4837160d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(unigramMLE([\"the\"]))\n",
        "print(bigramMLE([\"it\", \"is\"]))\n",
        "print(trigramMLE([\"agree\",\"it\",\"should\"]))\n",
        "print(quadgramMLE([\"agree\", \"it\", \"should\", \"be\"]))"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.037663616620252915\n",
            "0.045366795366795366\n",
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct-1sqg6YhLI",
        "colab_type": "text"
      },
      "source": [
        "## Generator using Classical Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuiqxPX5KT_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Classical generator to generate new words using a n_gram model\n",
        "def MLE_Generator(n_gram, initial_sequence):\n",
        "  sentence = [\"<s>\"]\n",
        "  if(n_gram==1):\n",
        "    for i in range(20):\n",
        "      max_prob = 0\n",
        "      max_prob_list = list()\n",
        "      for j in vocab_dict.keys():\n",
        "        k = (unigramMLE([j]))\n",
        "        if(k>max_prob):\n",
        "          max_prob = k\n",
        "          max_prob_list = [j]\n",
        "        elif(k==max_prob):\n",
        "          max_prob_list.append(j)\n",
        "      samples = np.random.multinomial(20,[max_prob]*len(max_prob_list),size=1)\n",
        "      index, value = max(enumerate(samples), key = operator.itemgetter(1))\n",
        "      sentence.append(max_prob_list[index])\n",
        "  else:\n",
        "    sentence.extend(initial_sequence)\n",
        "    i = len(initial_sequence)\n",
        "    while(sentence[-1]!=\"</s>\" and i<20):\n",
        "      max_prob = 0\n",
        "      max_prob_list = list()\n",
        "      for j in vocab:\n",
        "        word_list = sentence[-n_gram+1:]\n",
        "        word_list.append(j)\n",
        "        if(n_gram==2):\n",
        "          k = bigramMLE(word_list)\n",
        "        elif(n_gram==3):\n",
        "          k = trigramMLE(word_list)\n",
        "        elif(n_gram==4):\n",
        "          k = quadgramMLE(word_list)\n",
        "        if(k>max_prob):\n",
        "          max_prob = k\n",
        "          max_prob_list = [j]\n",
        "        elif(k==max_prob):\n",
        "          max_prob_list.append(j)\n",
        "      samples = np.random.multinomial(20,[max_prob]*len(max_prob_list),size=1)\n",
        "      index, value = max(enumerate(samples), key = operator.itemgetter(1))\n",
        "      sentence.append(max_prob_list[index])\n",
        "      i+=1\n",
        "  \n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM-uzG0dYvNG",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of Classical Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0cRjb-enyuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to calculate sentence probability of given n_gram model\n",
        "def Probability(sentence, n_gram):\n",
        "  prob = 0\n",
        "  sentence = sentence.split(\" \")\n",
        "  if(n_gram==1):\n",
        "    fun = unigramMLE\n",
        "    sentence = sentence[1:-1]\n",
        "  elif(n_gram==2):\n",
        "    fun = bigramMLE\n",
        "  elif(n_gram==3):\n",
        "    fun = trigramMLE\n",
        "  else:\n",
        "    fun = quadgramMLE\n",
        "  \n",
        "  for i in range(len(sentence)-n_gram+1):\n",
        "    w = sentence[i:i+n_gram]\n",
        "    prob += math.log(fun(w),2)\n",
        "  \n",
        "  return math.pow(2,prob)\n",
        "\n",
        "\n",
        "# function to calculate perplexity of all sentences given n_gram model\n",
        "def Perplexity(sentences, n_gram):\n",
        "  tot_len = 0\n",
        "  prob = 0\n",
        "\n",
        "  for i in sentences:\n",
        "    try:\n",
        "      prob -= math.log(Probability(i, n_gram),2)\n",
        "    except:\n",
        "      prob -= 0\n",
        "    sentence = i.split(\" \")\n",
        "    tot_len += (len(sentence)-n_gram-1)\n",
        "  \n",
        "  return math.pow(2, prob/tot_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6xU5IW1ZSEH",
        "colab_type": "text"
      },
      "source": [
        "### Perplexity Calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhsgzgAXvPfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "aa02627e-7436-43ba-ee11-54d9b498cdf2"
      },
      "source": [
        "# Calculating perplexity of all sentences in test dataset\n",
        "\n",
        "print(\"unigram perplexity: \",Perplexity(test, 1))\n",
        "print(\"bigram perplexity: \",Perplexity(test, 2))\n",
        "print(\"trigram perplexity: \",Perplexity(test, 3))\n",
        "print(\"quadgram perplexity: \",Perplexity(test, 4))"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unigram perplexity:  589.1865432242371\n",
            "bigram perplexity:  58.26548504814822\n",
            "trigram perplexity:  6.053377064057492\n",
            "quadgram perplexity:  1.881737040439814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98O3RVizZWRM",
        "colab_type": "text"
      },
      "source": [
        "### Random Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuK3MHYY9Imj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "c567492c-39c9-43b9-ce31-a7fb76322365"
      },
      "source": [
        "# GENERATING RANDOM TEXT:\n",
        "# UNIGRAMS\n",
        "print(\"UNIGRAM: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(1,[])))\n",
        "# BIGRAM\n",
        "print(\"BIGRAM: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(2,[\"it\", \"is\"])))\n",
        "# TRIGRAM\n",
        "print(\"TRIGRAM: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(3,[\"there\", \"is\", \"something\"])))\n",
        "# QUADGRAM\n",
        "print(\"QUADGRAM1: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(4,[\"but\", \"there\", \"is\", \"no\"])))\n",
        "# ANOTHER QuADGRAM\n",
        "print(\"QUADGRAM2: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(4,[\"it\", \"is\", \"to\", \"be\"])))"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIGRAM:  <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n",
            "BIGRAM:  <s> it is a lot of the world </s>\n",
            "TRIGRAM:  <s> there is something that i can tell you that </s>\n",
            "QUADGRAM1:  <s> but there is no leadership at the top </s>\n",
            "QUADGRAM2:  <s> it is to be treated a little bit </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2L7WRJVPEiH",
        "colab_type": "text"
      },
      "source": [
        "Unigram Keeps on printing the word with maximum probability, as unigram does not keep an account of the context words.\n",
        "Bigram does better than unigram but still prints out sentences that make no sense.\n",
        "Trigram and Quadgram does much better than the previous two, printing sentences that make more sense and are readable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9pN6eBfY6TL",
        "colab_type": "text"
      },
      "source": [
        "# NEURAL APPROACH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBIXZnw_9NM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NEURAL APPROACH\n",
        "sequences = list()\n",
        "n_gram = 3\n",
        "for i in range(len(train)):\n",
        "  tokens = train[i].split(\" \")\n",
        "  if(len(tokens)>=n_gram):\n",
        "    for k in range(0,len(tokens)-n_gram+1):\n",
        "      sequences.append(tokens[k:k+n_gram])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b4xDIGGKTOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "2c116ce4-1dd7-4ca2-d5fe-4e915a1ff5ae"
      },
      "source": [
        "sequences[:10]"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<s>', \"they're\", 'walking'],\n",
              " [\"they're\", 'walking', '</s>'],\n",
              " ['<s>', 'i', 'killed'],\n",
              " ['i', 'killed', 'the'],\n",
              " ['killed', 'the', 'applause'],\n",
              " ['the', 'applause', '</s>'],\n",
              " ['<s>', 'it’s', 'a'],\n",
              " ['it’s', 'a', 'crooked'],\n",
              " ['a', 'crooked', 'outfit.look,'],\n",
              " ['crooked', 'outfit.look,', 'club']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-PUhje9KVdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "sequences_intEncoded = tokenizer.texts_to_sequences(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQoywzNBKffM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "368e97b4-3d42-4b9c-b3f9-95e380a0e2d9"
      },
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW_2qrE9M0dd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "97f77cbc-eef4-4c9f-8382-650c38fa9ee5"
      },
      "source": [
        "sequences_intEncoded[:10]"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   4,  366, 1327],\n",
              "       [ 366, 1327,    5],\n",
              "       [   4,    6,  511],\n",
              "       [   6,  511,    1],\n",
              "       [ 511,    1, 1908],\n",
              "       [   1, 1908,    5],\n",
              "       [   4,   26,    7],\n",
              "       [  26,    7,  951],\n",
              "       [   7,  951, 4507],\n",
              "       [ 951, 4507, 1093]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-My_5WoO5b9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03ba06d7-6196-4ea1-a081-ff0f85330fc3"
      },
      "source": [
        "sequences_intEncoded = np.array(sequences_intEncoded)\n",
        "np.shape(sequences)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131432, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXbBHGGgO8uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = sequences_intEncoded[:,:-1]\n",
        "y = sequences_intEncoded[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sSw15vXPEP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdkevxyrZDRT",
        "colab_type": "text"
      },
      "source": [
        "## VANILLA RNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4UAN71RPHZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "732d8b0e-5e2d-44fb-f7a3-9c9fd8d897da"
      },
      "source": [
        "# VANILLA RNN MODEL\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(vocab_size, 50, input_length=n_gram-1))\n",
        "model_rnn.add(SimpleRNN(25, return_sequences=True))\n",
        "model_rnn.add(SimpleRNN(25))\n",
        "model_rnn.add(Dense(50, activation='relu'))\n",
        "model_rnn.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model_rnn.summary()"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 2, 50)             509950    \n",
            "_________________________________________________________________\n",
            "simple_rnn_9 (SimpleRNN)     (None, 2, 25)             1900      \n",
            "_________________________________________________________________\n",
            "simple_rnn_10 (SimpleRNN)    (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 50)                1300      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10199)             520149    \n",
            "=================================================================\n",
            "Total params: 1,034,574\n",
            "Trainable params: 1,034,574\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j-ZPbvIQK90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "e40c5700-d7e7-4b55-8141-5d6fc4bf1d85"
      },
      "source": [
        "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_rnn = model_rnn.fit(X, y, batch_size=128, epochs=10)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "131432/131432 [==============================] - 98s 747us/step - loss: 6.2811 - acc: 0.1017\n",
            "Epoch 2/10\n",
            "131432/131432 [==============================] - 96s 728us/step - loss: 5.4971 - acc: 0.1518\n",
            "Epoch 3/10\n",
            "131432/131432 [==============================] - 95s 723us/step - loss: 5.1230 - acc: 0.1786\n",
            "Epoch 4/10\n",
            "131432/131432 [==============================] - 96s 728us/step - loss: 4.8673 - acc: 0.1978\n",
            "Epoch 5/10\n",
            "131432/131432 [==============================] - 95s 721us/step - loss: 4.6687 - acc: 0.2126\n",
            "Epoch 6/10\n",
            "131432/131432 [==============================] - 96s 727us/step - loss: 4.4980 - acc: 0.2248\n",
            "Epoch 7/10\n",
            "131432/131432 [==============================] - 97s 735us/step - loss: 4.3434 - acc: 0.2361\n",
            "Epoch 8/10\n",
            "131432/131432 [==============================] - 95s 726us/step - loss: 4.1995 - acc: 0.2470\n",
            "Epoch 9/10\n",
            "131432/131432 [==============================] - 92s 697us/step - loss: 4.0643 - acc: 0.2583\n",
            "Epoch 10/10\n",
            "131432/131432 [==============================] - 89s 675us/step - loss: 3.9388 - acc: 0.2669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AwE6M6882ih",
        "colab_type": "text"
      },
      "source": [
        "### Generating Random Text using Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whTGUhXdu69S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8eaa958c-11c6-48fd-b563-ff46463ea512"
      },
      "source": [
        "for i in range(5):\n",
        "  result = list()\n",
        "  text = test[random.randint(0,len(test))].split(\" \")\n",
        "  f = 0\n",
        "  if(len(text)>=n_gram):\n",
        "    result = text[:n_gram-1]\n",
        "    l = n_gram\n",
        "    while(result[-1]!=\"</s>\"):\n",
        "      encoded = tokenizer.texts_to_sequences([result[-n_gram+1:]])\n",
        "      encoded = np.array(encoded)\n",
        "      yhat = model_rnn.predict_classes(encoded, verbose=0)\n",
        "      out_word = ''\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        if index == yhat:\n",
        "          out_word = word\n",
        "          break\n",
        "      result.append(out_word)\n",
        "      l+=1\n",
        "    print(\" \".join(result))"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> let’s do it </s>\n",
            "<s> if you know what </s>\n",
            "<s> when you have to be a great job </s>\n",
            "<s> we’re going to be a great job </s>\n",
            "<s> now, i have a lot of money </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C55l1pj1ZJIN",
        "colab_type": "text"
      },
      "source": [
        "## LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrIBVXINPdlS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "347f7e71-c6f7-4625-a1de-6d62dc3271b6"
      },
      "source": [
        "# LSTM MODEL\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_size, 50, input_length=n_gram-1))\n",
        "model_lstm.add(LSTM(25, return_sequences=True))\n",
        "model_lstm.add(LSTM(25))\n",
        "model_lstm.add(Dense(50, activation='relu'))\n",
        "model_lstm.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model_lstm.summary()"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 2, 50)             509950    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 2, 25)             7600      \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 25)                5100      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 50)                1300      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10199)             520149    \n",
            "=================================================================\n",
            "Total params: 1,044,099\n",
            "Trainable params: 1,044,099\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOrTbaDFQAhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "b28d06c0-3201-40c6-cddd-c084bf0dcd33"
      },
      "source": [
        "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_lstm = model_lstm.fit(X, y, batch_size=128, epochs=10)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "131432/131432 [==============================] - 103s 784us/step - loss: 5.9425 - acc: 0.1161\n",
            "Epoch 2/10\n",
            "131432/131432 [==============================] - 99s 752us/step - loss: 5.5952 - acc: 0.1413\n",
            "Epoch 3/10\n",
            "131432/131432 [==============================] - 99s 752us/step - loss: 5.3575 - acc: 0.1612\n",
            "Epoch 4/10\n",
            "131432/131432 [==============================] - 100s 760us/step - loss: 5.1744 - acc: 0.1742\n",
            "Epoch 5/10\n",
            "131432/131432 [==============================] - 99s 757us/step - loss: 5.0386 - acc: 0.1840\n",
            "Epoch 6/10\n",
            "131432/131432 [==============================] - 100s 757us/step - loss: 4.9216 - acc: 0.1910\n",
            "Epoch 7/10\n",
            "131432/131432 [==============================] - 97s 740us/step - loss: 4.8120 - acc: 0.1990\n",
            "Epoch 8/10\n",
            "131432/131432 [==============================] - 97s 738us/step - loss: 4.7075 - acc: 0.2075\n",
            "Epoch 9/10\n",
            "131432/131432 [==============================] - 96s 732us/step - loss: 4.6058 - acc: 0.2140\n",
            "Epoch 10/10\n",
            "131432/131432 [==============================] - 96s 730us/step - loss: 4.5089 - acc: 0.2207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phr25DoC9UnU",
        "colab_type": "text"
      },
      "source": [
        "### Generating Random Text Using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTXieKurQZ0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e8ebad1b-2e03-4ee4-b029-ec6401054c68"
      },
      "source": [
        "for i in range(5):\n",
        "  result = list()\n",
        "  text = test[random.randint(0,len(test))].split(\" \")\n",
        "  f = 0\n",
        "  if(len(text)>=n_gram):\n",
        "    result = text[:n_gram-1]\n",
        "    l = n_gram\n",
        "    while(result[-1]!=\"</s>\"):\n",
        "      encoded = tokenizer.texts_to_sequences([result[-n_gram+1:]])\n",
        "      encoded = np.array(encoded)\n",
        "      yhat = model_lstm.predict_classes(encoded, verbose=0)\n",
        "      out_word = ''\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        if index == yhat:\n",
        "          out_word = word\n",
        "          break\n",
        "      result.append(out_word)\n",
        "      l+=1\n",
        "    print(\" \".join(result))"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> they have to be a lot of people </s>\n",
            "<s> he was going to be a lot of people </s>\n",
            "<s> it’s a lot of people </s>\n",
            "<s> you’re going to be a lot of people </s>\n",
            "<s> sorry </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFHHXdMk9PVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyCk_QggSha1",
        "colab_type": "text"
      },
      "source": [
        "### Does Neural Approach perform better than Classical Approach. Why or why not?\n",
        "Yes.\n",
        "In neural approach the network learns how to use words, while in classical approach it is just a function of probability.\n",
        "In neural approach, context is remembered for a longer duration than in classical methods, where context in remembered only upto certain words (0 for unigram, 1 for bigram, 2 for trigram and 3 for quadgram). Also LSTM model remembers more than vanilla RNN, due to vanishing gradients in Vanilla RNNs.\n",
        "Neural approach also takes into account the similarity between words and hence can output a more sensible sentence than classical methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q80RSs53UFSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}